{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8063ad65",
   "metadata": {},
   "source": [
    "## **Urdu Sentence Annotation for Hate Speech Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784bc2e",
   "metadata": {},
   "source": [
    "### **Objective**\n",
    "The objective of this assignment was to scrape Urdu comments from YouTube videos and label them as offensive, hateful, or neutral to build a dataset for hate speech detection. The dataset is meant to aid in training machine learning models for detecting hate speech in the Urdu language. The assignment involved several pipelining phases, including data collection, preprocessing, and labeling.\n",
    "\n",
    "The key objective of this assignment was to:\n",
    "\n",
    "- Collect 300 Urdu comments (100 from each category: offensive, hateful, and neutral).\n",
    "\n",
    "- Clean the collected text to remove irrelevant characters.\n",
    "\n",
    "- Label the comments based on their content into three categories: offensive, hateful, or neutral.\n",
    "\n",
    "- Validate the annotation with the help of a peer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff767e",
   "metadata": {},
   "source": [
    "### **Data Collection**\n",
    "\n",
    "The data collection was carried out by scraping YouTube comments using the YouTube Data API. Several YouTube video URLs were chosen from Urdu news, political, and cultural discussions, where comments might include offensive or hateful content. The process used in this project was:\n",
    "\n",
    "**API Setup:** The `googleapiclient.discovery` module was used to interact with the YouTube Data API. The API key was obtained via the Google Developer Console.\n",
    "\n",
    "**Video Selection:** Few YouTube videos were chosen based on their relevance to topics that could include hate speech or offensive language.\n",
    "\n",
    "**Comment Extraction:** The comments were extracted from each video by making API calls for each video ID. The API retrieves comments, and the script ensures that it collects a sufficient number of comments by handling pagination (nextPageToken).\n",
    "\n",
    "**Keeping only Nastaliq Urdu:** A cleaning function was applied to each comment, keeping only the relevant Urdu characters and removing non-Urdu characters.\n",
    "\n",
    "**Sampling:** 300 comments (100 per category) were sampled randomly and saved for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd552599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 771 Nastaliq Urdu comments! Saved to nastaliq_urdu_youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Setup YouTube API\n",
    "api_key = 'AIzaSyA5ilIVxIEb_XWXJL0kwj0qlpmTwGQjYqA'  # <-- Put your YouTube Data API v3 key here\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List of video URLs\n",
    "video_urls = [\n",
    "    \"https://www.youtube.com/watch?v=B85_wZF7XCY\",\n",
    "    \"https://www.youtube.com/watch?v=vSQeAgGUA80\",   \n",
    "    \"https://www.youtube.com/watch?v=sCjNi75u-y4\",   \n",
    "    \"https://www.youtube.com/watch?v=oHEs19lx0ZU\",   \n",
    "    \"https://www.youtube.com/watch?v=8LpGOBQxwhU\",   \n",
    "    \"https://www.youtube.com/watch?v=-0jcXcN7F_I\",\n",
    "    \"https://www.youtube.com/watch?v=WNfzFSSRGrU\",\n",
    "    \"https://www.youtube.com/watch?v=4zzsVCsTxGU\",   \n",
    "    \"https://www.youtube.com/watch?v=0f2RTcz9Dho\" \n",
    "]\n",
    "\n",
    "# Urdu script detector\n",
    "def is_urdu_nastaliq(text, threshold=0.6):\n",
    "    urdu_chars = re.findall(r'[\\u0600-\\u06FF]', text)\n",
    "    return len(urdu_chars) / max(len(text), 1) >= threshold\n",
    "\n",
    "comments = []\n",
    "\n",
    "# Fetch comments\n",
    "for video_url in video_urls:\n",
    "    video_id = video_url.split('v=')[1]\n",
    "    nextPageToken = None\n",
    "    try:\n",
    "        while True:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=nextPageToken,\n",
    "                maxResults=100,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                if comment and len(comment) > 15 and is_urdu_nastaliq(comment):\n",
    "                    comments.append(comment)\n",
    "\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if not nextPageToken:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments for {video_url}: {e}\")\n",
    "\n",
    "# Shuffle and pick top 300\n",
    "comments = list(set(comments))\n",
    "random.shuffle(comments)\n",
    "selected_comments = comments[:1000]\n",
    "\n",
    "# Save\n",
    "df = pd.DataFrame(selected_comments, columns=[\"comments\"])\n",
    "df.to_csv(\"nastaliq_urdu_youtube_comments.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Scraped {len(selected_comments)} Nastaliq Urdu comments! Saved to nastaliq_urdu_youtube_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ed923",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "The preprocessing of comments included:\n",
    "\n",
    "**Text Cleaning:** Using regular expressions, all non-relevant characters were removed, leaving only the Urdu characters.\n",
    "\n",
    "- Emojis were removed\n",
    "\n",
    "- Non-Urdu text and numbers were removed\n",
    "\n",
    "- Excess spaces were removed\n",
    "\n",
    "- Empty rows were dropped\n",
    "\n",
    "**Unicode Handling:** The text was cleaned to ensure that the dataset contains only valid Urdu characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcfcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Saved 1032 clean Urdu comments to preprocessed_urdu_youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"nastaliq_urdu_youtube_comments.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Emoji pattern\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "\"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Urdu characters range (removes Arabic letters not used in Urdu)\n",
    "urdu_allowed = r'\\u0600-\\u06FF'\n",
    "# Remove unwanted Arabic supplement, digits, Latin, etc.\n",
    "def preprocess_comment(text):\n",
    "    # Remove emojis\n",
    "    text = emoji_pattern.sub('', text)\n",
    "    # Remove English, digits, punctuation, etc.\n",
    "    text = re.sub(r'[a-zA-Z0-9@#%^&*()_+=\\[\\]{}|\\\\:;\"\\'<>,./?!~`–“”\\'\\\"]+', ' ', text)\n",
    "    # Remove Arabic Presentation Forms (non-Nastaliq Arabic glyphs)\n",
    "    text = re.sub(r'[\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF]', ' ', text)\n",
    "    # Keep only Nastaliq Urdu characters and spaces\n",
    "    text = re.sub(f'[^{urdu_allowed} ]+', ' ', text)\n",
    "    # Normalize whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned'] = df['comments'].astype(str).apply(preprocess_comment)\n",
    "\n",
    "# Drop empty rows after cleaning\n",
    "df = df[df['cleaned'].str.strip() != '']\n",
    "\n",
    "# Save\n",
    "df[['cleaned']].to_csv(\"preprocessed_urdu_youtube_comments.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Preprocessing complete. Saved {len(df)} clean Urdu comments to preprocessed_urdu_youtube_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17a075",
   "metadata": {},
   "source": [
    "### **Annotation**\n",
    "\n",
    "The manual annotation through this script was based on the following categories:\n",
    "\n",
    "**Offensive (o):** Comments that use profane language or derogatory remarks aimed at individuals or groups.\n",
    "\n",
    "**Hate Speech (h):** Comments that promote hatred or violence towards specific groups, often based on religion, ethnicity, or nationality.\n",
    "\n",
    "**Neutral (n):** Comments that do not contain harmful language, hate speech, or offensive content. These are simply informational or neutral statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ed489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'h' for Hate Speech, 'o' for Offensive, 'n' for Neutral.\n",
      "----------------------------------------------------------\n",
      "\n",
      "Comment 1:\n",
      "اپنا آئی ایم ایف والا قرضہ دے کر مرنا نہیں تو تمہارے حصے کا ہمیں بھرنا پڑے گا\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comment 2:\n",
      "ڈکی حرامخور ہیرا منڈی کی پیداوار ہے لعنت تیری نسل پر جھوٹے\n",
      "\n",
      "Comment 3:\n",
      "ایک میاں جہنم میں جائے گا اپنی بیوی کو منع نہیں کرتا بھائی جان میں جائے گا باپ باب جہنم میں جائے گا ماں جہنم میں جائے گی جو منع نہیں کرتی پردے کا باقی اپ کی موج ہے بے حیائی پھیلاتے جائیں اپ کو بہت ثواب ملے گا بے حیائی پھیلاتے جائیں بالکل منع نہ کریں\n",
      "\n",
      "Comment 4:\n",
      "شامیر بھی گانڈو ہے اور شام بھی بس فروگی اچھی ہے\n",
      "\n",
      "Comment 5:\n",
      "بہن سے کبھی ادھار مت لینا لیا تھا بار دے چکا ہوں ابھی بھی باقی ہے\n",
      "\n",
      "Comment 6:\n",
      "محنت تو بھکاری بھی کرتے ہیں اور ڈاکو بھی۔آخر اِنکی محنت کیوں کِسی کو نظر نہیں آتی ؟\n",
      "\n",
      "Comment 7:\n",
      "لکھتے قلم ختم ہو جائے گی لیکن میرے نبی کی شان کبھی کم نہیں ہوگی\n",
      "\n",
      "Comment 8:\n",
      "ڈکی بھائی آپ ولوگ شروع کرے ہم آپ کے ساتھ ہے\n",
      "\n",
      "Comment 9:\n",
      "ڈٹے رہو ڈکی بھائ\n",
      "\n",
      "Comment 10:\n",
      "اِنَرَبِّیِ یَفعَلُمَا یَشَاَءُ بیشک میرا رب جو چاھے، کر سکتا ھے\n",
      "\n",
      "Comment 11:\n",
      "ہر بےعزتی والے کمنٹس پر ڈھیٹ بن کر ہنس رہے ہیں\n",
      "\n",
      "Comment 12:\n",
      "آپ بی اپنا کام اچھا کرو ۔ ہر وقت دوسرے کی عزت اڑاتے ہو بس۔\n",
      "\n",
      "Comment 13:\n",
      "اللہ نہ کرے شہزادے آپ کو کچھ ھو اللہ اپنی امان میں رکھے آمین اللہ آمین\n",
      "\n",
      "Comment 14:\n",
      "اللہ تعالیٰ آپ کو اولاد کی نعمت عطاء فرمائے آمین\n",
      "\n",
      "Comment 15:\n",
      "لکھتے قلم ختم ہو جائے گی لیکن میرے نبی کی شان کبھی کم نہیں ہوگی ۔\n",
      "\n",
      "Comment 16:\n",
      "تم لوگوں کا آپس میں کیا ہوا ، لڑائی کیوں ہوئی ؟ اس کے بعد تم نے لوگوں نے ایک دوسرے کے خلاف کیا کیا ؟ میرا خیال ہے کہ یہ کسی بھی دیکھنے والے کا موضوع نہیں ہونا چاہئیے ۔۔۔ حقیقت اور سچ یہی ہے کہ ، تم لوگوں نے اپنے دیکھنے والوں کو دھوکہ دینے کی کوشش کی ہے ۔۔۔۔ اور اس کو پورا پلان کیا ہے ۔۔۔۔ اب اس میں کوئی کتنا قصوروار تھا ۔۔۔۔ یہ معنی نہیں رکھتا ۔۔۔۔ تم لوگوں نے مل کے پلان بنایا ۔۔۔۔ تو تم سب برابر کے دمہ دار ہو ۔۔۔ اس بات پر شرمندہ ہونے کی بجائے باقی کہانیاں سنا رہے ہو اور اصل قصور کو مان نہیں رہے ۔۔۔ ارے بھائی ہمیں کیا غرض کہ طلحہ ، ندیم یا شام کے درمیان لڑائی کیوں ہے ؟؟؟ ہمارا مطلب یہ کہ تم لوگوں کو شرم نہیں آئی دیکھنے والوں کے خلاف پلان بناتے ہوئے ۔۔۔۔۔ اس بات پر شرم کرو بے شرمو۔۔۔۔۔۔\n",
      "\n",
      "Comment 17:\n",
      "شام خبیث بھائی جان۔۔۔ اب آپ پکا وڑ گئے ہو اور یہ ویڈیو آپکے تابوت میں آخری کیل ہے۔\n",
      "\n",
      "Comment 18:\n",
      "تم سب بھڑوت کہا رہے ھو\n",
      "\n",
      "Comment 19:\n",
      "ان بھکاریوں سےبہتر دین کی معلومات میں اضافہ کریں۔\n",
      "\n",
      "Comment 20:\n",
      "سعد بھائی میں لاہور سے ہوں جب بھی لاہور آئیں پلیز مجھ سے ضرور ملئیے گا۔ میں نے آج ہی آپ کا چینل سبسکرائب کیا ہے۔ لو یو برو\n",
      "\n",
      "Comment 21:\n",
      "یار ان کتوں کی بھی لوگ سنتے ہیں حد ہے ویسے\n",
      "\n",
      "Comment 22:\n",
      "یہ دونوں چوتیے ہیں\n",
      "\n",
      "Comment 23:\n",
      "لوگوں نے ان سبسکرائب تو ایسے کیا جیسے کبھی اپ سے پیار ہی نہیں کیا تھا جب بھی ایک ملین ہوا اس نے اپنے بال کاٹے اس نے لوگوں کو بہت انٹرٹینمنٹ کیا\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "input_file = 'preprocessed_urdu_youtube_comments.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Add a new column for annotations\n",
    "df['label'] = ''\n",
    "\n",
    "print(\"Type 'h' for Hate Speech, 'o' for Offensive, 'n' for Neutral.\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "# Loop through each comment\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"\\nComment {idx + 1}:\")\n",
    "    print(row['cleaned'])\n",
    "\n",
    "    while True:\n",
    "        annotation = input(\"Enter annotation (h/o/n): \").strip().lower()\n",
    "        if annotation in ['h', 'o', 'n']:\n",
    "            df.at[idx, 'Annotation'] = annotation\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 'h', 'o', or 'n'.\")\n",
    "\n",
    "# Save the annotated file\n",
    "output_file = 'annotated_dataset.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nAnnotation complete! Saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167708e6",
   "metadata": {},
   "source": [
    "### **Validation**\n",
    "\n",
    "The annotated dataset was validated with the help of a peer and the final dataset file `annotated_data_validated.csv` was created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43711ec",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "The final dataset contains 300 comments, distributed as follows:\n",
    "\n",
    "- 100 Offensive Comments\n",
    "\n",
    "- 100 Hate Speech Comments\n",
    "\n",
    "- 100 Neutral Comments\n",
    "\n",
    "Each comment was carefully reviewed and assigned the appropriate label. The comments were saved in a `annotated_data_validated.csv` CSV file, with each comment paired with its corresponding label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rizzy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
